{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb Models\n",
    "\n",
    "Alex Bass (ujb3bu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Final Project Ungraded Assignment\n",
    "At this point in the course, you should be training and evaluating models. Please create a Jupyter Notebook containing a concise summary of your dataset (described in submission instructions).  \n",
    "\n",
    "At a minimum, the file should include a summary containing:\n",
    "\n",
    "- Number of records\n",
    "- Number of columns\n",
    "- Statistical summary of response variable\n",
    "- Statistical summary of potential predictor variables (if there are a large number of predictors, select the top 10)\n",
    "    - Note: Summarize categorical variables with counts and percentages for each level and summarize numerical variables with mean/quantiles/standard deviation.\n",
    "- Include up to five helpful graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for data\n",
    "schema = StructType() \\\n",
    "      .add(\"id\",StringType(),True) \\\n",
    "      .add(\"date_account_created\",StringType(),True) \\\n",
    "      .add(\"timestamp_first_active\",DoubleType(),True) \\\n",
    "      .add(\"date_first_booking\",StringType(),True) \\\n",
    "      .add(\"gender\",StringType(),True) \\\n",
    "      .add(\"age\",DoubleType(),True) \\\n",
    "      .add(\"signup_method\",StringType(),True) \\\n",
    "      .add(\"signup_flow\",IntegerType(),True) \\\n",
    "      .add(\"language\",StringType(),True) \\\n",
    "      .add(\"affiliate_channel\",StringType(),True) \\\n",
    "      .add(\"affiliate_provider\",StringType(),True) \\\n",
    "      .add(\"first_affiliate_tracked\",StringType(),True) \\\n",
    "      .add(\"signup_app\",StringType(),True) \\\n",
    "      .add(\"first_device_type\",StringType(),True) \\\n",
    "      .add(\"first_browser\",StringType(),True) \\\n",
    "      .add(\"country_destination\",StringType(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of columns\n",
    "response_col = \"country_destination\"\n",
    "id_col = \"id\"\n",
    "categorical_cols = [\"gender\", \"signup_method\", \"language\", \n",
    "                    \"affiliate_channel\", \"affiliate_provider\", \"first_affiliate_tracked\",\n",
    "                    \"signup_app\", \"first_device_type\", \"first_browser\"]\n",
    "numeric_cols = [\"timestamp_first_active\", \"age\", \"signup_flow\", ]\n",
    "date_cols = [\"date_account_created\", \"date_first_booking\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in json format\n",
    "df = spark.read.option(\"header\",True).csv(\"./data/train_users_2.csv\", schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for data\n",
    "schema_sessions = StructType() \\\n",
    "      .add(\"user_id\",StringType(),True) \\\n",
    "      .add(\"action\",StringType(),True) \\\n",
    "      .add(\"action_type\",StringType(),True) \\\n",
    "      .add(\"action_detail\",StringType(),True) \\\n",
    "      .add(\"device_type\",StringType(),True) \\\n",
    "      .add(\"secs_elapsed\",DoubleType(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in json format\n",
    "df_sessions = spark.read.option(\"header\",True) \\\n",
    "    .csv(\"./data/sessions.csv\", schema_sessions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate some session data to user level\n",
    "session_agg = df_sessions.groupby('user_id').agg(\n",
    "    fn.sum('secs_elapsed').alias('total_time_elapsed'),\n",
    "    fn.count('action').alias('total_num_actions'),\n",
    "    fn.countDistinct('action').alias('num_unique_actions')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join datasets - use left join to keep as user data\n",
    "# Doing inner join becuase high % missing session data for train data\n",
    "# But low % missing session data in test data\n",
    "df = df.join(session_agg,df.id ==  session_agg.user_id, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent missing per column\n",
    "df.agg(*[\n",
    "    (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing')\n",
    "    for c in df.columns\n",
    "]).show(vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of columns for initial model\n",
    "\n",
    "features = [\"age\", \"gender\", \"signup_method\", \n",
    "            \"language\", \"signup_app\", \n",
    "            \"total_time_elapsed\", \"total_num_actions\", \n",
    "            \"first_device_type\", \"date_account_created\"]\n",
    "\n",
    "df = df.select(features + ['country_destination', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of response variable (count and percentage)\n",
    "\n",
    "df.groupBy(\"country_destination\") \\\n",
    "  .count() \\\n",
    "  .withColumnRenamed('count', 'Count') \\\n",
    "  .withColumn('Percent', fn.round((fn.col('Count') / df.count()) * 100 , 2)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary column (booked place or did not)\n",
    "df = df.withColumn(\n",
    "    'booked',\n",
    "    fn.when((df.country_destination == 'NDF'), 0)\\\n",
    "    .otherwise(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of binary response variable (count and percentage)\n",
    "\n",
    "df.groupBy(\"booked\") \\\n",
    "  .count() \\\n",
    "  .withColumnRenamed('count', 'Count') \\\n",
    "  .withColumn('Percent', fn.round((fn.col('Count') / df.count()) * 100 , 2)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values by booked by column\n",
    "df.groupby(\"booked\").agg(*[\n",
    "    (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing')\n",
    "    for c in df.columns\n",
    "]).show(vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace age with nan if outside reasonable range\n",
    "df = df.withColumn(\n",
    "    'age_new', \\\n",
    "    fn.when((df.age > 100) | (df.age < 16), None)\\\n",
    "    .otherwise(df.age)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column indicating whether age is missing or not\n",
    "# Since % missing is higher for those that didn't book\n",
    "# may be a proxy for how effort user has put into updating profile\n",
    "df = df.withColumn(\n",
    "    'age_missing',\n",
    "    fn.when(df.age_new.isNull(), 1)\\\n",
    "    .otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"age_missing\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALEX ADDITION: Time Elapsed Since Account Creation\n",
    "#check device type\n",
    "df.groupby(\"first_device_type\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the `other` categories are a bit small so combining\n",
    "# Create binary column (booked place or did not)\n",
    "df = df.withColumn(\n",
    "    'first_device_type',\n",
    "    fn.when((df.first_device_type == 'Android Tablet'), 'Android Tablet')\\\n",
    "    .when((df.first_device_type == 'iPad'), 'iPad')\\\n",
    "    .when((df.first_device_type == 'iPhone'), 'iPhone')\\\n",
    "    .when((df.first_device_type == 'Windows Desktop'), 'Windows Desktop')\\\n",
    "    .when((df.first_device_type == 'Android Phone'), 'Android Phone')\\\n",
    "    .when((df.first_device_type == 'Mac Desktop'), 'Mac Desktop')\\\n",
    "    .otherwise(\"Other\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"first_device_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(fn.col(\"date_account_created\"),fn.year(fn.col(\"date_account_created\")).alias(\"year\")).groupby(\"year\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like all accounts were created in 2014, so going to use month instead. I thought there would be more variance here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also going to create a time elapsed variable to get at longer customers\n",
    "import seaborn as sns\n",
    "temp_data = df.select(fn.col(\"date_account_created\"),fn.month(fn.col(\"date_account_created\")).alias(\"month\")).toPandas()\n",
    "sns.histplot(data=temp_data, x=\"month\")\n",
    "plt.title('Months Accounts are created in Year 2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can treat this as a numeric variable since all months are in 2014.\n",
    "df = df.withColumn(\n",
    "    'date_account_created',\n",
    "    fn.month(fn.col(\"date_account_created\")).alias(\"month\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to rename the variable 'booked' to 'label'\n",
    "df = df.withColumn(\n",
    "    'label',\n",
    "    df['booked']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/test\n",
    "seed = 123\n",
    "train, test = df.randomSplit([.7,.3], seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for gender, signup method, language, signup app\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Gender\n",
    "gender_idx = StringIndexer(inputCol=\"gender\", outputCol=\"gender_idx\")\n",
    "gender_ohe = OneHotEncoder(inputCol=\"gender_idx\", outputCol=\"gender_vec\")\n",
    "\n",
    "# Signup method\n",
    "signup_method_idx = StringIndexer(inputCol=\"signup_method\", outputCol=\"signup_method_idx\")\n",
    "signup_method_ohe = OneHotEncoder(inputCol=\"signup_method_idx\", outputCol=\"signup_method_vec\")\n",
    "\n",
    "# Language\n",
    "language_idx = StringIndexer(inputCol=\"language\", outputCol=\"language_idx\")\n",
    "language_ohe = OneHotEncoder(inputCol=\"language_idx\", outputCol=\"language_vec\")\n",
    "\n",
    "# Signup app\n",
    "signup_app_idx = StringIndexer(inputCol=\"signup_app\", outputCol=\"signup_app_idx\")\n",
    "signup_app_ohe = OneHotEncoder(inputCol=\"signup_app_idx\", outputCol=\"signup_app_vec\")\n",
    "\n",
    "### Alex Additions ###\n",
    "# First Device Type - Perhaps people browsing on their computer may be more serious about buying vs casual browsing on phone\n",
    "device_idx = StringIndexer(inputCol=\"first_device_type\", outputCol=\"first_device_type_idx\")\n",
    "device_ohe = OneHotEncoder(inputCol=\"first_device_type_idx\", outputCol=\"first_device_type_vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with median\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Age\n",
    "imputer_age = Imputer(\n",
    "    inputCol='age_new', \n",
    "    outputCol='age_new_imputed'\n",
    "    ).setStrategy(\"median\")\n",
    "\n",
    "# Total time elapsed\n",
    "imputer_total_elapsed = Imputer(\n",
    "    inputCol='total_time_elapsed', \n",
    "    outputCol='total_time_elapsed_imputed'\n",
    "    ).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "features = [\"age_new_imputed\", \"age_missing\",\n",
    "            \"gender_vec\", \"signup_method_vec\", \"language_vec\", \"signup_app_vec\",\n",
    "             \"total_time_elapsed_imputed\", \"total_num_actions\", \"first_device_type_vec\", \"date_account_created\"]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features,\n",
    "                            outputCol=\"fts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all features\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "# Using maxabsscaler because some OHE features are sparse\n",
    "scaler = MaxAbsScaler(inputCol=\"fts\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logistic regression model\n",
    "max_iterations = 10\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=max_iterations,\n",
    "                        featuresCol = 'features',\n",
    "                        labelCol = 'label'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[gender_idx, gender_ohe, \n",
    "                           signup_method_idx, signup_method_ohe,\n",
    "                           language_idx, language_ohe,\n",
    "                           signup_app_idx, signup_app_ohe, device_idx, device_ohe,\n",
    "                           imputer_age, imputer_total_elapsed,\n",
    "                           assembler, scaler,\n",
    "                           lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [1, 0.5, 0.1, 0.01, 0]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "import time\n",
    "t0 = time.time()\n",
    "cv_model_lr = crossval.fit(df)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(\"elastic net 1\", \"to elastic net 0.01\")\n",
    "print(cv_model_lr.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol = 'features',\n",
    "    labelCol = 'label'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(stages=[gender_idx, gender_ohe, \n",
    "                           signup_method_idx, signup_method_ohe,\n",
    "                           language_idx, language_ohe,\n",
    "                           signup_app_idx, signup_app_ohe, device_idx, device_ohe,\n",
    "                           imputer_age, imputer_total_elapsed,\n",
    "                           assembler, scaler,\n",
    "                           rf])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 20, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [3,5,6]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "\n",
    "t0 = time.time()\n",
    "cv_model_rf = crossval.fit(df)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(cv_model_rf.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(\n",
    "    featuresCol = 'features',\n",
    "    labelCol = 'label'\n",
    ")\n",
    "\n",
    "pipeline_bayes = Pipeline(stages=[gender_idx, gender_ohe, \n",
    "                           signup_method_idx, signup_method_ohe,\n",
    "                           language_idx, language_ohe,\n",
    "                           signup_app_idx, signup_app_ohe, device_idx, device_ohe,\n",
    "                           imputer_age, imputer_total_elapsed,\n",
    "                           assembler, scaler,\n",
    "                           nb])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0, 0.5, 1, 5]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_bayes,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "\n",
    "t0 = time.time()\n",
    "cv_model_bayes = crossval.fit(df)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(cv_model_bayes.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol = 'features',\n",
    "    labelCol = 'label',\n",
    "    maxIter = 5\n",
    ")\n",
    "\n",
    "pipeline_gbt = Pipeline(stages=[gender_idx, gender_ohe, \n",
    "                           signup_method_idx, signup_method_ohe,\n",
    "                           language_idx, language_ohe,\n",
    "                           signup_app_idx, signup_app_ohe, device_idx, device_ohe,\n",
    "                           imputer_age, imputer_total_elapsed,\n",
    "                           assembler, scaler,\n",
    "                           gbt])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.max_depth, [3, 5, 6]) \\\n",
    "    .addGrid(gbt.minWeightFractionPerNode, [0, 0.01, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "\n",
    "t0 = time.time()\n",
    "cv_model_gbt = crossval.fit(df)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(cv_model_gbt.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Model\" : [\n",
    "        \"Lasso\",\n",
    "        \"Ridge\",\n",
    "        \"Naive Bayes\",\n",
    "        \"Random Forest\",\n",
    "        \"GBT\"\n",
    "    ],\n",
    "    \"AUC\" : [\n",
    "        cv_model_lr.avgMetrics[0],\n",
    "        cv_model_lr.avgMetrics[max(len(cv_model_lr.avgMetrics))],\n",
    "        max(cv_model_nb.avgMetrics),\n",
    "        max(cv_model_rf.avgMetrics),\n",
    "        max(cv_model_gbt.avgMetrics)\n",
    "    ]\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
