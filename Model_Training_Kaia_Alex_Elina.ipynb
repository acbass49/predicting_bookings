{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb Models\n",
    "\n",
    "Alex Bass (ujb3bu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Final Project Ungraded Assignment\n",
    "At this point in the course, you should be training and evaluating models. Please create a Jupyter Notebook containing a concise summary of your dataset (described in submission instructions).  \n",
    "\n",
    "At a minimum, the file should include a summary containing:\n",
    "\n",
    "- Number of records\n",
    "- Number of columns\n",
    "- Statistical summary of response variable\n",
    "- Statistical summary of potential predictor variables (if there are a large number of predictors, select the top 10)\n",
    "    - Note: Summarize categorical variables with counts and percentages for each level and summarize numerical variables with mean/quantiles/standard deviation.\n",
    "- Include up to five helpful graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for data\n",
    "schema = StructType() \\\n",
    "      .add(\"id\",StringType(),True) \\\n",
    "      .add(\"date_account_created\",StringType(),True) \\\n",
    "      .add(\"timestamp_first_active\",DoubleType(),True) \\\n",
    "      .add(\"date_first_booking\",StringType(),True) \\\n",
    "      .add(\"gender\",StringType(),True) \\\n",
    "      .add(\"age\",DoubleType(),True) \\\n",
    "      .add(\"signup_method\",StringType(),True) \\\n",
    "      .add(\"signup_flow\",IntegerType(),True) \\\n",
    "      .add(\"language\",StringType(),True) \\\n",
    "      .add(\"affiliate_channel\",StringType(),True) \\\n",
    "      .add(\"affiliate_provider\",StringType(),True) \\\n",
    "      .add(\"first_affiliate_tracked\",StringType(),True) \\\n",
    "      .add(\"signup_app\",StringType(),True) \\\n",
    "      .add(\"first_device_type\",StringType(),True) \\\n",
    "      .add(\"first_browser\",StringType(),True) \\\n",
    "      .add(\"country_destination\",StringType(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of columns\n",
    "response_col = \"country_destination\"\n",
    "id_col = \"id\"\n",
    "categorical_cols = [\"gender\", \"signup_method\", \"language\", \n",
    "                    \"affiliate_channel\", \"affiliate_provider\", \"first_affiliate_tracked\",\n",
    "                    \"signup_app\", \"first_device_type\", \"first_browser\"]\n",
    "numeric_cols = [\"timestamp_first_active\", \"age\", \"signup_flow\", ]\n",
    "date_cols = [\"date_account_created\", \"date_first_booking\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in json format\n",
    "df = spark.read.option(\"header\",True).csv(\"train_users_2.csv\", schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for data\n",
    "schema_sessions = StructType() \\\n",
    "      .add(\"user_id\",StringType(),True) \\\n",
    "      .add(\"action\",StringType(),True) \\\n",
    "      .add(\"action_type\",StringType(),True) \\\n",
    "      .add(\"action_detail\",StringType(),True) \\\n",
    "      .add(\"device_type\",StringType(),True) \\\n",
    "      .add(\"secs_elapsed\",DoubleType(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in json format\n",
    "df_sessions = spark.read.option(\"header\",True) \\\n",
    "    .csv(\"sessions.csv\", schema_sessions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate some session data to user level\n",
    "session_agg = df_sessions.groupby('user_id').agg(\n",
    "    fn.sum('secs_elapsed').alias('total_time_elapsed'),\n",
    "    fn.count('action').alias('total_num_actions'),\n",
    "    fn.countDistinct('action').alias('num_unique_actions')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join datasets - use left join to keep as user data\n",
    "# Doing inner join becuase high % missing session data for train data\n",
    "# But low % missing session data in test data\n",
    "df = df.join(session_agg,df.id ==  session_agg.user_id, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------\n",
      " id_missing                      | 0.0                  \n",
      " date_account_created_missing    | 0.0                  \n",
      " timestamp_first_active_missing  | 0.0                  \n",
      " date_first_booking_missing      | 0.6101876312402628   \n",
      " gender_missing                  | 0.0                  \n",
      " age_missing                     | 0.43687597371807896  \n",
      " signup_method_missing           | 0.0                  \n",
      " signup_flow_missing             | 0.0                  \n",
      " language_missing                | 0.0                  \n",
      " affiliate_channel_missing       | 0.0                  \n",
      " affiliate_provider_missing      | 0.0                  \n",
      " first_affiliate_tracked_missing | 0.004091309354467221 \n",
      " signup_app_missing              | 0.0                  \n",
      " first_device_type_missing       | 0.0                  \n",
      " first_browser_missing           | 0.0                  \n",
      " country_destination_missing     | 0.0                  \n",
      " user_id_missing                 | 0.0                  \n",
      " total_time_elapsed_missing      | 0.01604010025062652  \n",
      " total_num_actions_missing       | 0.0                  \n",
      " num_unique_actions_missing      | 0.0                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Percent missing per column\n",
    "df.agg(*[\n",
    "    (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing')\n",
    "    for c in df.columns\n",
    "]).show(vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of columns for initial model\n",
    "\n",
    "features = [\"age\", \"gender\", \"signup_method\", \n",
    "            \"language\", \"signup_app\", \n",
    "            \"total_time_elapsed\", \"total_num_actions\", \n",
    "            \"first_device_type\", \"date_account_created\"]\n",
    "\n",
    "df = df.select(features + ['country_destination', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------+\n",
      "|country_destination|Count|Percent|\n",
      "+-------------------+-----+-------+\n",
      "|                 NL|  247|   0.33|\n",
      "|                 PT|   83|   0.11|\n",
      "|                 AU|  152|   0.21|\n",
      "|                 CA|  440|    0.6|\n",
      "|                 GB|  731|   0.99|\n",
      "|              other| 3655|   4.95|\n",
      "|                 DE|  250|   0.34|\n",
      "|                 ES|  707|   0.96|\n",
      "|                 US|20095|  27.22|\n",
      "|                 FR| 1435|   1.94|\n",
      "|                NDF|45041|  61.02|\n",
      "|                 IT|  979|   1.33|\n",
      "+-------------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistical summary of response variable (count and percentage)\n",
    "\n",
    "df.groupBy(\"country_destination\") \\\n",
    "  .count() \\\n",
    "  .withColumnRenamed('count', 'Count') \\\n",
    "  .withColumn('Percent', fn.round((fn.col('Count') / df.count()) * 100 , 2)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary column (booked place or did not)\n",
    "df = df.withColumn(\n",
    "    'booked',\n",
    "    fn.when((df.country_destination == 'NDF'), 0)\\\n",
    "    .otherwise(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+\n",
      "|booked|Count|Percent|\n",
      "+------+-----+-------+\n",
      "|     1|28774|  38.98|\n",
      "|     0|45041|  61.02|\n",
      "+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistical summary of binary response variable (count and percentage)\n",
    "\n",
    "df.groupBy(\"booked\") \\\n",
    "  .count() \\\n",
    "  .withColumnRenamed('count', 'Count') \\\n",
    "  .withColumn('Percent', fn.round((fn.col('Count') / df.count()) * 100 , 2)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------\n",
      " booked                       | 1                    \n",
      " age_missing                  | 0.2110933481615347   \n",
      " gender_missing               | 0.0                  \n",
      " signup_method_missing        | 0.0                  \n",
      " language_missing             | 0.0                  \n",
      " signup_app_missing           | 0.0                  \n",
      " total_time_elapsed_missing   | 0.012685062904010613 \n",
      " total_num_actions_missing    | 0.0                  \n",
      " first_device_type_missing    | 0.0                  \n",
      " date_account_created_missing | 0.0                  \n",
      " country_destination_missing  | 0.0                  \n",
      " id_missing                   | 0.0                  \n",
      " booked_missing               | 0.0                  \n",
      "-RECORD 1--------------------------------------------\n",
      " booked                       | 0                    \n",
      " age_missing                  | 0.5811149841255745   \n",
      " gender_missing               | 0.0                  \n",
      " signup_method_missing        | 0.0                  \n",
      " language_missing             | 0.0                  \n",
      " signup_app_missing           | 0.0                  \n",
      " total_time_elapsed_missing   | 0.018183432872271976 \n",
      " total_num_actions_missing    | 0.0                  \n",
      " first_device_type_missing    | 0.0                  \n",
      " date_account_created_missing | 0.0                  \n",
      " country_destination_missing  | 0.0                  \n",
      " id_missing                   | 0.0                  \n",
      " booked_missing               | 0.0                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Missing values by booked by column\n",
    "df.groupby(\"booked\").agg(*[\n",
    "    (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing')\n",
    "    for c in df.columns\n",
    "]).show(vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace age with nan if outside reasonable range\n",
    "df = df.withColumn(\n",
    "    'age_new', \\\n",
    "    fn.when((df.age > 100) | (df.age < 16), None)\\\n",
    "    .otherwise(df.age)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column indicating whether age is missing or not\n",
    "# Since % missing is higher for those that didn't book\n",
    "# may be a proxy for how effort user has put into updating profile\n",
    "df = df.withColumn(\n",
    "    'age_missing',\n",
    "    fn.when(df.age_new.isNull(), 1)\\\n",
    "    .otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|age_missing|count|\n",
      "+-----------+-----+\n",
      "|          1|32974|\n",
      "|          0|40841|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"age_missing\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'gender',\n",
       " 'signup_method',\n",
       " 'language',\n",
       " 'signup_app',\n",
       " 'total_time_elapsed',\n",
       " 'total_num_actions',\n",
       " 'first_device_type',\n",
       " 'date_account_created',\n",
       " 'country_destination',\n",
       " 'id',\n",
       " 'booked',\n",
       " 'age_new',\n",
       " 'age_missing']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "| first_device_type|count|\n",
      "+------------------+-----+\n",
      "|    Android Tablet|  701|\n",
      "|              iPad| 5238|\n",
      "|            iPhone|10961|\n",
      "|   Windows Desktop|23395|\n",
      "|SmartPhone (Other)|   30|\n",
      "|     Android Phone| 1979|\n",
      "|       Mac Desktop|28029|\n",
      "|     Other/Unknown| 2993|\n",
      "|   Desktop (Other)|  489|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ALEX ADDITION: Time Elapsed Since Account Creation\n",
    "#check device type\n",
    "df.groupby(\"first_device_type\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the `other` categories are a bit small so combining\n",
    "# Create binary column (booked place or did not)\n",
    "df = df.withColumn(\n",
    "    'first_device_type',\n",
    "    fn.when((df.first_device_type == 'Android Tablet'), 'Android Tablet')\\\n",
    "    .when((df.first_device_type == 'iPad'), 'iPad')\\\n",
    "    .when((df.first_device_type == 'iPhone'), 'iPhone')\\\n",
    "    .when((df.first_device_type == 'Windows Desktop'), 'Windows Desktop')\\\n",
    "    .when((df.first_device_type == 'Android Phone'), 'Android Phone')\\\n",
    "    .when((df.first_device_type == 'Mac Desktop'), 'Mac Desktop')\\\n",
    "    .otherwise(\"Other\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|first_device_type|count|\n",
      "+-----------------+-----+\n",
      "|   Android Tablet|  701|\n",
      "|             iPad| 5238|\n",
      "|           iPhone|10961|\n",
      "|  Windows Desktop|23395|\n",
      "|            Other| 3512|\n",
      "|    Android Phone| 1979|\n",
      "|      Mac Desktop|28029|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"first_device_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|2014|73815|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(fn.col(\"date_account_created\"),fn.year(fn.col(\"date_account_created\")).alias(\"year\")).groupby(\"year\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like all accounts were created in 2014, so going to use month instead. I thought there would be more variance here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Months Accounts are created in Year 2014')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Also going to create a time elapsed variable to get at longer customers\n",
    "import seaborn as sns\n",
    "temp_data = df.select(fn.col(\"date_account_created\"),fn.month(fn.col(\"date_account_created\")).alias(\"month\")).toPandas()\n",
    "sns.histplot(data=temp_data, x=\"month\")\n",
    "plt.title('Months Accounts are created in Year 2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can treat this as a numeric variable since all months are in 2014.\n",
    "df = df.withColumn(\n",
    "    'date_account_created',\n",
    "    fn.month(fn.col(\"date_account_created\")).alias(\"month\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to rename the variable 'booked' to 'label'\n",
    "df = df.withColumn(\n",
    "    'label',\n",
    "    df['booked']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/test\n",
    "seed = 123\n",
    "train, test = df.randomSplit([.7,.3], seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for gender, signup method, language, signup app\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Gender\n",
    "gender_idx = StringIndexer(inputCol=\"gender\", outputCol=\"gender_idx\")\n",
    "gender_ohe = OneHotEncoder(inputCol=\"gender_idx\", outputCol=\"gender_vec\")\n",
    "\n",
    "# Signup method\n",
    "signup_method_idx = StringIndexer(inputCol=\"signup_method\", outputCol=\"signup_method_idx\")\n",
    "signup_method_ohe = OneHotEncoder(inputCol=\"signup_method_idx\", outputCol=\"signup_method_vec\")\n",
    "\n",
    "# Language\n",
    "language_idx = StringIndexer(inputCol=\"language\", outputCol=\"language_idx\")\n",
    "language_ohe = OneHotEncoder(inputCol=\"language_idx\", outputCol=\"language_vec\")\n",
    "\n",
    "# Signup app\n",
    "signup_app_idx = StringIndexer(inputCol=\"signup_app\", outputCol=\"signup_app_idx\")\n",
    "signup_app_ohe = OneHotEncoder(inputCol=\"signup_app_idx\", outputCol=\"signup_app_vec\")\n",
    "\n",
    "### Alex Additions ###\n",
    "# First Device Type - Perhaps people browsing on their computer may be more serious about buying vs casual browsing on phone\n",
    "device_idx = StringIndexer(inputCol=\"first_device_type\", outputCol=\"first_device_type_idx\")\n",
    "device_ohe = OneHotEncoder(inputCol=\"first_device_type_idx\", outputCol=\"first_device_type_vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with median\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Age\n",
    "imputer_age = Imputer(\n",
    "    inputCol='age_new', \n",
    "    outputCol='age_new_imputed'\n",
    "    ).setStrategy(\"median\")\n",
    "\n",
    "# Total time elapsed\n",
    "imputer_total_elapsed = Imputer(\n",
    "    inputCol='total_time_elapsed', \n",
    "    outputCol='total_time_elapsed_imputed'\n",
    "    ).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "features = [\"age_new_imputed\", \"age_missing\",\n",
    "            \"gender_vec\", \"signup_method_vec\", \"language_vec\", \"signup_app_vec\",\n",
    "             \"total_time_elapsed_imputed\", \"total_num_actions\", \"first_device_type_vec\", \"date_account_created\"]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features,\n",
    "                            outputCol=\"fts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all features\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "# Using maxabsscaler because some OHE features are sparse\n",
    "scaler = MaxAbsScaler(inputCol=\"fts\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logistic regression model\n",
    "max_iterations = 10\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=max_iterations,\n",
    "                        featuresCol = 'features',\n",
    "                        labelCol = 'label'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[gender_idx, gender_ohe, \n",
    "                           signup_method_idx, signup_method_ohe,\n",
    "                           language_idx, language_ohe,\n",
    "                           signup_app_idx, signup_app_ohe, device_idx, device_ohe,\n",
    "                           imputer_age, imputer_total_elapsed,\n",
    "                           assembler, scaler,\n",
    "                           lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "paramGrid [{Param(parent='LogisticRegression_5eb5731d22f4', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0}, {Param(parent='LogisticRegression_5eb5731d22f4', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5}, {Param(parent='LogisticRegression_5eb5731d22f4', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.1}, {Param(parent='LogisticRegression_5eb5731d22f4', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.01}, {Param(parent='LogisticRegression_5eb5731d22f4', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0}] \n",
      "\n",
      "len(paramGrid): 5\n",
      "------------------------------\n",
      "train time: 1209.661659002304\n",
      "------------------------------\n",
      "elastic net 1 to elastic net 0.01\n",
      "[0.7579579105271874, 0.7579583856346788, 0.7579607975131721, 0.7579566035734275, 0.757957627286034]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [1, 0.5, 0.1, 0.01, 0]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "import time\n",
    "t0 = time.time()\n",
    "cv_model_lr = crossval.fit(df)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(\"elastic net 1\", \"to elastic net 0.01\")\n",
    "print(cv_model_lr.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol = 'features',\n",
    "    labelCol = 'label'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "paramGrid [{Param(parent='RandomForestClassifier_d7f26639698d', name='numTrees', doc='Number of trees to train (>= 1).'): 5, Param(parent='RandomForestClassifier_d7f26639698d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3}, {Param(parent='RandomForestClassifier_d7f26639698d', name='numTrees', doc='Number of trees to train (>= 1).'): 5, Param(parent='RandomForestClassifier_d7f26639698d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5}, {Param(parent='RandomForestClassifier_d7f26639698d', name='numTrees', doc='Number of trees to train (>= 1).'): 5, Param(parent='RandomForestClassifier_d7f26639698d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6}, {Param(parent='RandomForestClassifier_d7f26639698d', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_d7f26639698d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3}, {Param(parent='RandomForestClassifier_d7f26639698d', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_d7f26639698d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5}, {Param(parent='RandomForestClassifier_d7f26639698d', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_d7f26639698d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6}, {Param(parent='RandomForestClassifier_d7f26639698d', name='numTrees', doc='Number of trees to train (>= 1).'): 50, Param(parent='RandomForestClassifier_d7f26639698d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3}, {Param(parent='RandomForestClassifier_d7f26639698d', name='numTrees', doc='Number of trees to train (>= 1).'): 50, Param(parent='RandomForestClassifier_d7f26639698d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5}, {Param(parent='RandomForestClassifier_d7f26639698d', name='numTrees', doc='Number of trees to train (>= 1).'): 50, Param(parent='RandomForestClassifier_d7f26639698d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6}] \n",
      "\n",
      "len(paramGrid): 9\n",
      "------------------------------\n",
      "train time: 2190.589357852936\n",
      "------------------------------\n",
      "[0.7246021928186646, 0.7547392023570099, 0.7577555072231432, 0.7390204459211531, 0.75686251740297, 0.758729062550062, 0.7487032374277903, 0.7574710871879899, 0.7595009328100761]\n"
     ]
    }
   ],
   "source": [
    "pipeline_rf = Pipeline(stages=[gender_idx, gender_ohe, \n",
    "                           signup_method_idx, signup_method_ohe,\n",
    "                           language_idx, language_ohe,\n",
    "                           signup_app_idx, signup_app_ohe, device_idx, device_ohe,\n",
    "                           imputer_age, imputer_total_elapsed,\n",
    "                           assembler, scaler,\n",
    "                           rf])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 20, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [3,5,6]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "\n",
    "t0 = time.time()\n",
    "cv_model_rf = crossval.fit(df)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(cv_model_rf.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "paramGrid [{Param(parent='NaiveBayes_7b1702a4521d', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.0}, {Param(parent='NaiveBayes_7b1702a4521d', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.5}, {Param(parent='NaiveBayes_7b1702a4521d', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 1.0}, {Param(parent='NaiveBayes_7b1702a4521d', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 5.0}] \n",
      "\n",
      "len(paramGrid): 4\n",
      "------------------------------\n",
      "train time: 692.2205185890198\n",
      "------------------------------\n",
      "[0.6877912235297616, 0.6877998617984563, 0.6878148800562481, 0.6879137950430848]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(\n",
    "    featuresCol = 'features',\n",
    "    labelCol = 'label'\n",
    ")\n",
    "\n",
    "pipeline_bayes = Pipeline(stages=[gender_idx, gender_ohe, \n",
    "                           signup_method_idx, signup_method_ohe,\n",
    "                           language_idx, language_ohe,\n",
    "                           signup_app_idx, signup_app_ohe, device_idx, device_ohe,\n",
    "                           imputer_age, imputer_total_elapsed,\n",
    "                           assembler, scaler,\n",
    "                           nb])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0, 0.5, 1, 5]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_bayes,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "\n",
    "t0 = time.time()\n",
    "cv_model_bayes = crossval.fit(df)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(cv_model_bayes.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GBTClassifier' object has no attribute 'max_depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-30a2758e0971>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Set up the parameter grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mparamGrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParamGridBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0maddGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0maddGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminWeightFractionPerNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GBTClassifier' object has no attribute 'max_depth'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol = 'features',\n",
    "    labelCol = 'label',\n",
    "    maxIter = 5\n",
    ")\n",
    "\n",
    "pipeline_gbt = Pipeline(stages=[gender_idx, gender_ohe, \n",
    "                           signup_method_idx, signup_method_ohe,\n",
    "                           language_idx, language_ohe,\n",
    "                           signup_app_idx, signup_app_ohe, device_idx, device_ohe,\n",
    "                           imputer_age, imputer_total_elapsed,\n",
    "                           assembler, scaler,\n",
    "                           gbt])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.max_depth, [3, 5, 6]) \\ # ER need to change AttributeError: 'GBTClassifier' object has no attribute 'max_depth'\n",
    "    .addGrid(gbt.minWeightFractionPerNode, [0, 0.01, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "\n",
    "t0 = time.time()\n",
    "cv_model_gbt = crossval.fit(df)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(cv_model_gbt.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-808b02e3790e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"AUC\" : [\n\u001b[1;32m     12\u001b[0m         \u001b[0mcv_model_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgMetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcv_model_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgMetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_model_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgMetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_model_nb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgMetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_model_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgMetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ER to check TypeError: 'int' object is not iterable\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Model\" : [\n",
    "        \"Lasso\",\n",
    "        \"Ridge\",\n",
    "        \"Naive Bayes\",\n",
    "        \"Random Forest\",\n",
    "        \"GBT\"\n",
    "    ],\n",
    "    \"AUC\" : [\n",
    "        cv_model_lr.avgMetrics[0],\n",
    "        cv_model_lr.avgMetrics[max(len(cv_model_lr.avgMetrics))],\n",
    "        max(cv_model_nb.avgMetrics),\n",
    "        max(cv_model_rf.avgMetrics),\n",
    "        max(cv_model_gbt.avgMetrics)\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
