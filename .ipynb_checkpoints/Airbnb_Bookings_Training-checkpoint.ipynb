{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb Models\n",
    "\n",
    "Alex Bass (ujb3bu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Final Project Ungraded Assignment\n",
    "At this point in the course, you should be training and evaluating models. Please create a Jupyter Notebook containing a concise summary of your dataset (described in submission instructions).  \n",
    "\n",
    "At a minimum, the file should include a summary containing:\n",
    "\n",
    "- Number of records\n",
    "- Number of columns\n",
    "- Statistical summary of response variable\n",
    "- Statistical summary of potential predictor variables (if there are a large number of predictors, select the top 10)\n",
    "    - Note: Summarize categorical variables with counts and percentages for each level and summarize numerical variables with mean/quantiles/standard deviation.\n",
    "- Include up to five helpful graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Parquet Files\n",
    "*Note: Don't need to read in a schema because this information is stored in the parquet file and applied when loaded*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in json format\n",
    "train = spark.read.option(\"header\",True).parquet(\"./data_preprocessed/train_data.parquet\")\n",
    "test = spark.read.option(\"header\",True).parquet(\"./data_preprocessed/test_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick edits before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|language| count|\n",
      "+--------+------+\n",
      "|      en|144481|\n",
      "|      pl|    41|\n",
      "|      pt|   157|\n",
      "|      ko|   532|\n",
      "|      cs|    20|\n",
      "|      tr|    48|\n",
      "|      de|   523|\n",
      "|      is|     4|\n",
      "|      es|   646|\n",
      "|      hr|     1|\n",
      "|      el|    19|\n",
      "|      it|   355|\n",
      "|      sv|    90|\n",
      "|      nl|    72|\n",
      "|      hu|    13|\n",
      "|      ca|     5|\n",
      "|      ru|   276|\n",
      "|      th|    21|\n",
      "|      no|    20|\n",
      "|      zh|  1137|\n",
      "|      fr|   817|\n",
      "|      ja|   148|\n",
      "|      id|    17|\n",
      "|      da|    42|\n",
      "|      fi|    11|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupby(\"language\").count().show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|language|count|\n",
      "+--------+-----+\n",
      "|      en|61833|\n",
      "|      pl|   13|\n",
      "|      pt|   83|\n",
      "|      ko|  215|\n",
      "|      cs|   12|\n",
      "|      tr|   16|\n",
      "|      de|  209|\n",
      "|      is|    1|\n",
      "|      es|  269|\n",
      "|      hr|    1|\n",
      "|      el|    5|\n",
      "|      it|  159|\n",
      "|      sv|   32|\n",
      "|      nl|   25|\n",
      "|      hu|    5|\n",
      "|      ru|  113|\n",
      "|      th|    3|\n",
      "|      no|   10|\n",
      "|      zh|  495|\n",
      "|      fr|  355|\n",
      "|      ja|   77|\n",
      "|      id|    5|\n",
      "|      da|   16|\n",
      "|      fi|    3|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.groupby(\"language\").count().show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are pretty low numbers of some of these languages that are not in both datasets (e.g. `ca`), so going to make a new language variable where the biggest languages have their own category and everything else is considered `other`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\n",
    "    'language',\n",
    "    fn.when((train.language == 'en'), 'en')\\\n",
    "    .when((train.language == 'es'), 'es')\\\n",
    "    .when((train.language == 'zh'), 'zh')\\\n",
    "    .when((train.language == 'fr'), 'fr')\\\n",
    "    .when((train.language == 'de'), 'de')\\\n",
    "    .when((train.language == 'ko'), 'ko')\\\n",
    "    .when((train.language == 'it'), 'it')\\\n",
    "    .when((train.language == 'ru'), 'ru')\\\n",
    "    .otherwise(\"other\")\n",
    ")\n",
    "\n",
    "test = test.withColumn(\n",
    "    'language',\n",
    "    fn.when((test.language == 'en'), 'en')\\\n",
    "    .when((test.language == 'es'), 'es')\\\n",
    "    .when((test.language == 'zh'), 'zh')\\\n",
    "    .when((test.language == 'fr'), 'fr')\\\n",
    "    .when((test.language == 'de'), 'de')\\\n",
    "    .when((test.language == 'ko'), 'ko')\\\n",
    "    .when((test.language == 'it'), 'it')\\\n",
    "    .when((test.language == 'ru'), 'ru')\\\n",
    "    .otherwise(\"other\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to rename the variable 'booked' to 'label'\n",
    "train = train.withColumn(\n",
    "    'label',\n",
    "    fn.when((train.country_destination == 'NDF'), 0)\\\n",
    "    .otherwise(1)\n",
    ")\n",
    "\n",
    "test = test.withColumn(\n",
    "    'label',\n",
    "    fn.when((test.country_destination == 'NDF'), 0)\\\n",
    "    .otherwise(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for gender, signup method, language, signup app\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Gender\n",
    "gender_idx = StringIndexer(inputCol=\"gender\", outputCol=\"gender_idx\")\n",
    "gender_ohe = OneHotEncoder(inputCol=\"gender_idx\", outputCol=\"gender_vec\")\n",
    "\n",
    "# Signup method\n",
    "signup_method_idx = StringIndexer(inputCol=\"signup_method\", outputCol=\"signup_method_idx\")\n",
    "signup_method_ohe = OneHotEncoder(inputCol=\"signup_method_idx\", outputCol=\"signup_method_vec\")\n",
    "\n",
    "# Language\n",
    "language_idx = StringIndexer(inputCol=\"language\", outputCol=\"language_idx\")\n",
    "language_ohe = OneHotEncoder(inputCol=\"language_idx\", outputCol=\"language_vec\")\n",
    "\n",
    "# Signup app\n",
    "signup_app_idx = StringIndexer(inputCol=\"signup_app\", outputCol=\"signup_app_idx\")\n",
    "signup_app_ohe = OneHotEncoder(inputCol=\"signup_app_idx\", outputCol=\"signup_app_vec\")\n",
    "\n",
    "# First Device Type - Perhaps people browsing on their computer may be more serious about buying vs casual browsing on phone\n",
    "device_idx = StringIndexer(inputCol=\"first_device_type\", outputCol=\"first_device_type_idx\")\n",
    "device_ohe = OneHotEncoder(inputCol=\"first_device_type_idx\", outputCol=\"first_device_type_vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with median\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Age\n",
    "imputer_age = Imputer(\n",
    "    inputCol='age_new', \n",
    "    outputCol='age_new_imputed'\n",
    "    ).setStrategy(\"median\")\n",
    "\n",
    "# Total time elapsed\n",
    "imputer_total_elapsed = Imputer(\n",
    "    inputCol='total_time_elapsed', \n",
    "    outputCol='total_time_elapsed_imputed'\n",
    "    ).setStrategy(\"median\")\n",
    "\n",
    "# # Total actions\n",
    "imputer_total_num_actions = Imputer(\n",
    "    inputCol='total_num_actions', \n",
    "    outputCol='total_num_actions_imputed'\n",
    "    ).setMissingValue(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "features = [\n",
    "    \"age_new_imputed\", \n",
    "    \"age_missing\",\n",
    "    \"gender_vec\", \n",
    "    \"signup_method_vec\", \n",
    "    \"language_vec\", \n",
    "    \"signup_app_vec\",\n",
    "    \"total_time_elapsed_imputed\", \n",
    "    'total_num_actions_imputed', \n",
    "    \"first_device_type_vec\"]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features,\n",
    "                            outputCol=\"fts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all features\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "# Using maxabsscaler because some OHE features are sparse\n",
    "scaler = MaxAbsScaler(inputCol=\"fts\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logistic regression model\n",
    "max_iterations = 10\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lasso = LogisticRegression(maxIter=max_iterations,\n",
    "                        featuresCol = 'features',\n",
    "                        labelCol = 'label',\n",
    "                        elasticNetParam=1\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline_lasso = Pipeline(stages=[\n",
    "    gender_idx, \n",
    "    gender_ohe, \n",
    "    signup_method_idx,\n",
    "    signup_method_ohe,\n",
    "    signup_app_idx, \n",
    "    signup_app_ohe,\n",
    "    device_idx, \n",
    "    device_ohe,\n",
    "    language_idx, \n",
    "    language_ohe,\n",
    "    imputer_age, \n",
    "    imputer_total_elapsed, \n",
    "    imputer_total_num_actions, \n",
    "    assembler, \n",
    "    scaler, \n",
    "    lasso])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 4.809465169906616\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "model_lasso = pipeline_lasso.fit(train)\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 4.415439128875732\n"
     ]
    }
   ],
   "source": [
    "# Setup logistic regression model\n",
    "max_iterations = 10\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "ridge = LogisticRegression(maxIter=max_iterations,\n",
    "                        featuresCol = 'features',\n",
    "                        labelCol = 'label',\n",
    "                        elasticNetParam=0\n",
    "                       )\n",
    "\n",
    "# Configure pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline_ridge = Pipeline(stages=[\n",
    "    gender_idx, \n",
    "    gender_ohe, \n",
    "    signup_method_idx,\n",
    "    signup_method_ohe,\n",
    "    signup_app_idx, \n",
    "    signup_app_ohe,\n",
    "    device_idx, \n",
    "    device_ohe,\n",
    "    language_idx, \n",
    "    language_ohe,\n",
    "    imputer_age, \n",
    "    imputer_total_elapsed, \n",
    "    imputer_total_num_actions, \n",
    "    assembler, \n",
    "    scaler, \n",
    "    lasso])\n",
    "\n",
    "t0 = time.time()\n",
    "model_ridge = pipeline_ridge.fit(train)\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol = 'features',\n",
    "    labelCol = 'label'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "paramGrid [{Param(parent='RandomForestClassifier_4637e8ee2521', name='numTrees', doc='Number of trees to train (>= 1).'): 5, Param(parent='RandomForestClassifier_4637e8ee2521', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3}, {Param(parent='RandomForestClassifier_4637e8ee2521', name='numTrees', doc='Number of trees to train (>= 1).'): 5, Param(parent='RandomForestClassifier_4637e8ee2521', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5}, {Param(parent='RandomForestClassifier_4637e8ee2521', name='numTrees', doc='Number of trees to train (>= 1).'): 5, Param(parent='RandomForestClassifier_4637e8ee2521', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6}, {Param(parent='RandomForestClassifier_4637e8ee2521', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_4637e8ee2521', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3}, {Param(parent='RandomForestClassifier_4637e8ee2521', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_4637e8ee2521', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5}, {Param(parent='RandomForestClassifier_4637e8ee2521', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_4637e8ee2521', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6}, {Param(parent='RandomForestClassifier_4637e8ee2521', name='numTrees', doc='Number of trees to train (>= 1).'): 50, Param(parent='RandomForestClassifier_4637e8ee2521', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3}, {Param(parent='RandomForestClassifier_4637e8ee2521', name='numTrees', doc='Number of trees to train (>= 1).'): 50, Param(parent='RandomForestClassifier_4637e8ee2521', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5}, {Param(parent='RandomForestClassifier_4637e8ee2521', name='numTrees', doc='Number of trees to train (>= 1).'): 50, Param(parent='RandomForestClassifier_4637e8ee2521', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6}] \n",
      "\n",
      "len(paramGrid): 9\n",
      "------------------------------\n",
      "train time: 106.03145956993103\n",
      "------------------------------\n",
      "[0.7163251409893358, 0.724100967982193, 0.7265767045640328, 0.7213575990533146, 0.7277062343323581, 0.7315061670965411, 0.7208661988950793, 0.7278677369610093, 0.7315595347576958]\n"
     ]
    }
   ],
   "source": [
    "pipeline_rf = Pipeline(stages=[\n",
    "    gender_idx, \n",
    "    gender_ohe, \n",
    "    signup_method_idx,\n",
    "    signup_method_ohe,\n",
    "    signup_app_idx, \n",
    "    signup_app_ohe,\n",
    "    device_idx, \n",
    "    device_ohe,\n",
    "    language_idx, \n",
    "    language_ohe,\n",
    "    imputer_age, \n",
    "    imputer_total_elapsed, \n",
    "    imputer_total_num_actions, \n",
    "    assembler, \n",
    "    scaler, \n",
    "    rf])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 20, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [3,5,6]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "\n",
    "t0 = time.time()\n",
    "cv_model_rf = crossval.fit(train)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(cv_model_rf.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "paramGrid [{Param(parent='NaiveBayes_669eef610a45', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.0}, {Param(parent='NaiveBayes_669eef610a45', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.5}, {Param(parent='NaiveBayes_669eef610a45', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 1.0}, {Param(parent='NaiveBayes_669eef610a45', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 5.0}] \n",
      "\n",
      "len(paramGrid): 4\n",
      "------------------------------\n",
      "train time: 37.93347787857056\n",
      "------------------------------\n",
      "[0.6743650970573876, 0.6743822631119618, 0.6743786795652509, 0.674403451482228]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(\n",
    "    featuresCol = 'features',\n",
    "    labelCol = 'label'\n",
    ")\n",
    "\n",
    "pipeline_bayes = Pipeline(stages=[\n",
    "    gender_idx, \n",
    "    gender_ohe, \n",
    "    signup_method_idx,\n",
    "    signup_method_ohe,\n",
    "    signup_app_idx, \n",
    "    signup_app_ohe,\n",
    "    device_idx, \n",
    "    device_ohe,\n",
    "    language_idx, \n",
    "    language_ohe,\n",
    "    imputer_age, \n",
    "    imputer_total_elapsed, \n",
    "    imputer_total_num_actions, \n",
    "    assembler, \n",
    "    scaler, \n",
    "    nb\n",
    "])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0, 0.5, 1, 5]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_bayes,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "\n",
    "t0 = time.time()\n",
    "cv_model_bayes = crossval.fit(train)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(cv_model_bayes.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "paramGrid [{Param(parent='GBTClassifier_00811aebe68c', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3, Param(parent='GBTClassifier_00811aebe68c', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0}, {Param(parent='GBTClassifier_00811aebe68c', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3, Param(parent='GBTClassifier_00811aebe68c', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.01}, {Param(parent='GBTClassifier_00811aebe68c', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3, Param(parent='GBTClassifier_00811aebe68c', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.1}, {Param(parent='GBTClassifier_00811aebe68c', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='GBTClassifier_00811aebe68c', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0}, {Param(parent='GBTClassifier_00811aebe68c', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='GBTClassifier_00811aebe68c', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.01}, {Param(parent='GBTClassifier_00811aebe68c', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='GBTClassifier_00811aebe68c', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.1}, {Param(parent='GBTClassifier_00811aebe68c', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6, Param(parent='GBTClassifier_00811aebe68c', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0}, {Param(parent='GBTClassifier_00811aebe68c', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6, Param(parent='GBTClassifier_00811aebe68c', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.01}, {Param(parent='GBTClassifier_00811aebe68c', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6, Param(parent='GBTClassifier_00811aebe68c', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.1}] \n",
      "\n",
      "len(paramGrid): 9\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol = 'features',\n",
    "    labelCol = 'label',\n",
    "    maxIter = 5\n",
    ")\n",
    "\n",
    "pipeline_gbt = Pipeline(stages=[\n",
    "    gender_idx, \n",
    "    gender_ohe, \n",
    "    signup_method_idx,\n",
    "    signup_method_ohe,\n",
    "    signup_app_idx, \n",
    "    signup_app_ohe,\n",
    "    device_idx, \n",
    "    device_ohe,\n",
    "    language_idx, \n",
    "    language_ohe,\n",
    "    imputer_age, \n",
    "    imputer_total_elapsed, \n",
    "    imputer_total_num_actions, \n",
    "    assembler, \n",
    "    scaler, \n",
    "    gbt\n",
    "])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [3, 5, 6]) \\\n",
    "    .addGrid(gbt.minWeightFractionPerNode, [0, 0.01, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "\n",
    "t0 = time.time()\n",
    "cv_model_gbt = crossval.fit(train)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(cv_model_gbt.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Model\" : [\n",
    "        \"Lasso\",\n",
    "        \"Ridge\",\n",
    "        \"Naive Bayes\",\n",
    "        \"Random Forest\",\n",
    "        \"GBT\"\n",
    "    ],\n",
    "    \"Training AUC\" : [\n",
    "        cv_model_lr.avgMetrics[0],\n",
    "        max(cv_model_bayes.avgMetrics),\n",
    "        max(cv_model_rf.avgMetrics),\n",
    "        max(cv_model_gbt.avgMetrics)\n",
    "    ]\n",
    "}).sort_values(\"AUC\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge.save('./models/ridge.model')\n",
    "model_lasso.save('./models/lasso.model')\n",
    "cv_model_bayes.save('./models/naive_bayes.model')\n",
    "cv_model_rf.save('./models/random_forest.model')\n",
    "cv_model_gbt.save('./models/gbt.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
