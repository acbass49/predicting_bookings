{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Parquet Files\n",
    "*Note: Don't need to read in a schema because this information is stored in the parquet file and applied when loaded*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in json format\n",
    "train = spark.read.option(\"header\",True).parquet(\"./data_preprocessed/train_data.parquet\")\n",
    "test = spark.read.option(\"header\",True).parquet(\"./data_preprocessed/test_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating One-Hot-Encoded Matricies for the Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for gender, signup method, language, signup app\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Gender\n",
    "gender_idx = StringIndexer(inputCol=\"gender\", outputCol=\"gender_idx\")\n",
    "gender_ohe = OneHotEncoder(inputCol=\"gender_idx\", outputCol=\"gender_vec\")\n",
    "\n",
    "# Signup method\n",
    "signup_method_idx = StringIndexer(inputCol=\"signup_method\", outputCol=\"signup_method_idx\")\n",
    "signup_method_ohe = OneHotEncoder(inputCol=\"signup_method_idx\", outputCol=\"signup_method_vec\")\n",
    "\n",
    "# Language\n",
    "language_idx = StringIndexer(inputCol=\"language\", outputCol=\"language_idx\")\n",
    "language_ohe = OneHotEncoder(inputCol=\"language_idx\", outputCol=\"language_vec\")\n",
    "\n",
    "# Signup app\n",
    "signup_app_idx = StringIndexer(inputCol=\"signup_app\", outputCol=\"signup_app_idx\")\n",
    "signup_app_ohe = OneHotEncoder(inputCol=\"signup_app_idx\", outputCol=\"signup_app_vec\")\n",
    "\n",
    "# First Device Type - Perhaps people browsing on their computer may be more serious about buying vs casual browsing on phone\n",
    "device_idx = StringIndexer(inputCol=\"first_device_type\", outputCol=\"first_device_type_idx\")\n",
    "device_ohe = OneHotEncoder(inputCol=\"first_device_type_idx\", outputCol=\"first_device_type_vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Age and Online Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with median\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Age\n",
    "imputer_age = Imputer(\n",
    "    inputCol='age_new', \n",
    "    outputCol='age_new_imputed'\n",
    "    ).setStrategy(\"median\")\n",
    "\n",
    "# Total time elapsed\n",
    "imputer_total_elapsed = Imputer(\n",
    "    inputCol='total_time_elapsed', \n",
    "    outputCol='total_time_elapsed_imputed'\n",
    "    ).setStrategy(\"median\")\n",
    "\n",
    "# # Total actions\n",
    "imputer_total_num_actions = Imputer(\n",
    "    inputCol='total_num_actions', \n",
    "    outputCol='total_num_actions_imputed'\n",
    "    ).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting and Scaling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "features = [\n",
    "    \"age_new_imputed\", \n",
    "    \"age_missing\",\n",
    "    \"gender_vec\", \n",
    "    \"signup_method_vec\", \n",
    "    \"language_vec\", \n",
    "    \"signup_app_vec\",\n",
    "    \"total_time_elapsed_imputed\", \n",
    "    'total_num_actions_imputed', \n",
    "    \"first_device_type_vec\"]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features,\n",
    "                            outputCol=\"fts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all features\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "# Using maxabsscaler because some OHE features are sparse\n",
    "scaler = MaxAbsScaler(inputCol=\"fts\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building First Pipeline for Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 12.152521133422852\n"
     ]
    }
   ],
   "source": [
    "# Setup logistic regression model\n",
    "max_iterations = 10\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lasso = LogisticRegression(maxIter=max_iterations,\n",
    "                        featuresCol = 'features',\n",
    "                        labelCol = 'label',\n",
    "                        elasticNetParam=1\n",
    "                       )\n",
    "\n",
    "# Configure pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline_lasso = Pipeline(stages=[\n",
    "    gender_idx, \n",
    "    gender_ohe, \n",
    "    signup_method_idx,\n",
    "    signup_method_ohe,\n",
    "    signup_app_idx, \n",
    "    signup_app_ohe,\n",
    "    device_idx, \n",
    "    device_ohe,\n",
    "    language_idx, \n",
    "    language_ohe,\n",
    "    imputer_age, \n",
    "    imputer_total_elapsed, \n",
    "    imputer_total_num_actions, \n",
    "    assembler, \n",
    "    scaler, \n",
    "    lasso])\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "model_lasso = pipeline_lasso.fit(train)\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 5.351033687591553\n"
     ]
    }
   ],
   "source": [
    "# Setup logistic regression model\n",
    "max_iterations = 10\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "ridge = LogisticRegression(maxIter=max_iterations,\n",
    "                        featuresCol = 'features',\n",
    "                        labelCol = 'label',\n",
    "                        elasticNetParam=0\n",
    "                       )\n",
    "\n",
    "# Configure pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline_ridge = Pipeline(stages=[\n",
    "    gender_idx, \n",
    "    gender_ohe, \n",
    "    signup_method_idx,\n",
    "    signup_method_ohe,\n",
    "    signup_app_idx, \n",
    "    signup_app_ohe,\n",
    "    device_idx, \n",
    "    device_ohe,\n",
    "    language_idx, \n",
    "    language_ohe,\n",
    "    imputer_age, \n",
    "    imputer_total_elapsed, \n",
    "    imputer_total_num_actions, \n",
    "    assembler, \n",
    "    scaler, \n",
    "    lasso])\n",
    "\n",
    "t0 = time.time()\n",
    "model_ridge = pipeline_ridge.fit(train)\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol = 'features',\n",
    "    labelCol = 'label'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "paramGrid [{Param(parent='RandomForestClassifier_7872cbbe10d4', name='numTrees', doc='Number of trees to train (>= 1).'): 5, Param(parent='RandomForestClassifier_7872cbbe10d4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3}, {Param(parent='RandomForestClassifier_7872cbbe10d4', name='numTrees', doc='Number of trees to train (>= 1).'): 5, Param(parent='RandomForestClassifier_7872cbbe10d4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5}, {Param(parent='RandomForestClassifier_7872cbbe10d4', name='numTrees', doc='Number of trees to train (>= 1).'): 5, Param(parent='RandomForestClassifier_7872cbbe10d4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6}, {Param(parent='RandomForestClassifier_7872cbbe10d4', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_7872cbbe10d4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3}, {Param(parent='RandomForestClassifier_7872cbbe10d4', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_7872cbbe10d4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5}, {Param(parent='RandomForestClassifier_7872cbbe10d4', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_7872cbbe10d4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6}, {Param(parent='RandomForestClassifier_7872cbbe10d4', name='numTrees', doc='Number of trees to train (>= 1).'): 50, Param(parent='RandomForestClassifier_7872cbbe10d4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3}, {Param(parent='RandomForestClassifier_7872cbbe10d4', name='numTrees', doc='Number of trees to train (>= 1).'): 50, Param(parent='RandomForestClassifier_7872cbbe10d4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5}, {Param(parent='RandomForestClassifier_7872cbbe10d4', name='numTrees', doc='Number of trees to train (>= 1).'): 50, Param(parent='RandomForestClassifier_7872cbbe10d4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6}] \n",
      "\n",
      "len(paramGrid): 9\n",
      "------------------------------\n",
      "train time: 116.47787404060364\n",
      "------------------------------\n",
      "[0.7095203287672253, 0.7250814210504155, 0.7273430493303342, 0.7206460483792773, 0.7291132133978999, 0.7307965970977066, 0.7192960633170156, 0.7272355330153084, 0.7316752383065199]\n"
     ]
    }
   ],
   "source": [
    "pipeline_rf = Pipeline(stages=[\n",
    "    gender_idx, \n",
    "    gender_ohe, \n",
    "    signup_method_idx,\n",
    "    signup_method_ohe,\n",
    "    signup_app_idx, \n",
    "    signup_app_ohe,\n",
    "    device_idx, \n",
    "    device_ohe,\n",
    "    language_idx, \n",
    "    language_ohe,\n",
    "    imputer_age, \n",
    "    imputer_total_elapsed, \n",
    "    imputer_total_num_actions, \n",
    "    assembler, \n",
    "    scaler, \n",
    "    rf])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 20, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [3,5,6]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "\n",
    "t0 = time.time()\n",
    "cv_model_rf = crossval.fit(train)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(cv_model_rf.avgMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "paramGrid [{Param(parent='NaiveBayes_9d6189695c22', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.0}, {Param(parent='NaiveBayes_9d6189695c22', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.5}, {Param(parent='NaiveBayes_9d6189695c22', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 1.0}, {Param(parent='NaiveBayes_9d6189695c22', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 5.0}] \n",
      "\n",
      "len(paramGrid): 4\n",
      "------------------------------\n",
      "train time: 36.54611682891846\n",
      "------------------------------\n",
      "[0.6742347141291045, 0.6742311507788784, 0.6742266566009952, 0.6742417471759576]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(\n",
    "    featuresCol = 'features',\n",
    "    labelCol = 'label'\n",
    ")\n",
    "\n",
    "pipeline_bayes = Pipeline(stages=[\n",
    "    gender_idx, \n",
    "    gender_ohe, \n",
    "    signup_method_idx,\n",
    "    signup_method_ohe,\n",
    "    signup_app_idx, \n",
    "    signup_app_ohe,\n",
    "    device_idx, \n",
    "    device_ohe,\n",
    "    language_idx, \n",
    "    language_ohe,\n",
    "    imputer_age, \n",
    "    imputer_total_elapsed, \n",
    "    imputer_total_num_actions, \n",
    "    assembler, \n",
    "    scaler, \n",
    "    nb\n",
    "])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0, 0.5, 1, 5]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_bayes,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "\n",
    "t0 = time.time()\n",
    "cv_model_bayes = crossval.fit(train)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(cv_model_bayes.avgMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "paramGrid [{Param(parent='GBTClassifier_3fbadd496b77', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3, Param(parent='GBTClassifier_3fbadd496b77', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0}, {Param(parent='GBTClassifier_3fbadd496b77', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3, Param(parent='GBTClassifier_3fbadd496b77', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.01}, {Param(parent='GBTClassifier_3fbadd496b77', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3, Param(parent='GBTClassifier_3fbadd496b77', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.1}, {Param(parent='GBTClassifier_3fbadd496b77', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='GBTClassifier_3fbadd496b77', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0}, {Param(parent='GBTClassifier_3fbadd496b77', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='GBTClassifier_3fbadd496b77', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.01}, {Param(parent='GBTClassifier_3fbadd496b77', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='GBTClassifier_3fbadd496b77', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.1}, {Param(parent='GBTClassifier_3fbadd496b77', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6, Param(parent='GBTClassifier_3fbadd496b77', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0}, {Param(parent='GBTClassifier_3fbadd496b77', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6, Param(parent='GBTClassifier_3fbadd496b77', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.01}, {Param(parent='GBTClassifier_3fbadd496b77', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6, Param(parent='GBTClassifier_3fbadd496b77', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.1}] \n",
      "\n",
      "len(paramGrid): 9\n",
      "------------------------------\n",
      "train time: 119.90655040740967\n",
      "------------------------------\n",
      "[0.7288888689027198, 0.729054354887057, 0.7200716964626415, 0.7375086278863451, 0.7356577692692411, 0.7217382531784802, 0.738226395799405, 0.73655784127876, 0.7224594931741651]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol = 'features',\n",
    "    labelCol = 'label',\n",
    "    maxIter = 5\n",
    ")\n",
    "\n",
    "pipeline_gbt = Pipeline(stages=[\n",
    "    gender_idx, \n",
    "    gender_ohe, \n",
    "    signup_method_idx,\n",
    "    signup_method_ohe,\n",
    "    signup_app_idx, \n",
    "    signup_app_ohe,\n",
    "    device_idx, \n",
    "    device_ohe,\n",
    "    language_idx, \n",
    "    language_ohe,\n",
    "    imputer_age, \n",
    "    imputer_total_elapsed, \n",
    "    imputer_total_num_actions, \n",
    "    assembler, \n",
    "    scaler, \n",
    "    gbt\n",
    "])\n",
    "\n",
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [3, 5, 6]) \\\n",
    "    .addGrid(gbt.minWeightFractionPerNode, [0, 0.01, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "print('-'*30)\n",
    "print('paramGrid', paramGrid, '\\n')\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=1)\n",
    "\n",
    "t0 = time.time()\n",
    "cv_model_gbt = crossval.fit(train)\n",
    "print(\"train time:\", time.time() - t0)\n",
    "print('-'*30)\n",
    "print(cv_model_gbt.avgMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge.save('./models/ridge.model')\n",
    "model_lasso.save('./models/lasso.model')\n",
    "cv_model_bayes.bestModel.save('./models/naive_bayes.model')\n",
    "cv_model_rf.bestModel.save('./models/random_forest.model')\n",
    "cv_model_gbt.bestModel.save('./models/gbt.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
