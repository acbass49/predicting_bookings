{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, NaiveBayes, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading a custom function from this gist: https://gist.github.com/HerveMignot/a8f18e52b31d11c21132ef624249a887\n",
    "def plot_confusion_matrix(ddf, labelCol, predictionCol, title=None, normalize=False):\n",
    "    '''Plot a Confusion Matrix in a scalable way\n",
    "    Confusion matrix is computed with Spark to leverage scalability.\n",
    "    Only aggregated data are sent back to Python, doing the final formatting on the matrix locally.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ddf: Spark DataFrame (pyspark.sql.dataframe.DataFrame)\n",
    "    labelCol: label column name (string)\n",
    "    predictionCol: prediction column name (string)\n",
    "    title: title string to be added\n",
    "    normalize : bool, {'all', 'index', 'columns'}, or {0, 1}, default False\n",
    "        Normalize by dividing all values by the sum of values.\n",
    "        - If passed 'all' or `True`, will normalize over all values.\n",
    "        - If passed 'index' will normalize over each row.\n",
    "        - If passed 'columns' will normalize over each column.\n",
    "        - If margins is `True`, will also normalize margin values.\n",
    "        (see pandas.crosstab() for additional details)\n",
    "    Returns\n",
    "    -------\n",
    "    Confusion matrix as matplotlib.axes._subplots.AxesSubplot\n",
    "    '''\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "    if title:\n",
    "        ax = plt.axes()\n",
    "        ax.set_title(title)\n",
    "\n",
    "    mat = (ddf.groupby([labelCol, predictionCol]).count().orderBy([labelCol, predictionCol])\n",
    "              .toPandas())\n",
    "\n",
    "    return sns.heatmap(pd.crosstab(mat[labelCol], mat[predictionCol], mat['count'], aggfunc='sum', normalize=normalize),\n",
    "                       annot=True, fmt='d' if not normalize else '.2%',\n",
    "                       cbar_kws={'format': FuncFormatter(lambda x, pos: '{:.0%}'.format(x))} if normalize else None,\n",
    "                       cmap=plt.cm.Blues)\n",
    "\n",
    "# Monkey patching a pyspark dataframe method\n",
    "pyspark.sql.dataframe.DataFrame.plot_confusion_matrix = plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in json format\n",
    "train = spark.read.option(\"header\",True).parquet(\"./data_preprocessed/train_data.parquet\")\n",
    "test = spark.read.option(\"header\",True).parquet(\"./data_preprocessed/test_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = PipelineModel.load('./models/ridge.model')\n",
    "model_lasso = PipelineModel.load('./models/lasso.model')\n",
    "model_bayes = PipelineModel.load('./models/naive_bayes.model')\n",
    "model_rf = PipelineModel.load('./models/random_forest.model')\n",
    "model_gbt = PipelineModel.load('./models/gbt.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_ridge,\n",
    "    model_lasso,\n",
    "    model_bayes,\n",
    "    model_rf,\n",
    "    model_gbt\n",
    "]\n",
    "\n",
    "ClassEval = BinaryClassificationEvaluator()\n",
    "OtherClassEval = MulticlassClassificationEvaluator()\n",
    "\n",
    "training_auc = [ClassEval.evaluate(model.transform(train)) for model in models]\n",
    "test_auc = [ClassEval.evaluate(model.transform(test)) for model in models]\n",
    "\n",
    "training_acc = [OtherClassEval.evaluate(model.transform(train), {OtherClassEval.metricName: \"accuracy\"}) for model in models]\n",
    "test_acc = [OtherClassEval.evaluate(model.transform(test), {OtherClassEval.metricName: \"accuracy\"}) for model in models]\n",
    "\n",
    "training_f1 = [OtherClassEval.evaluate(model.transform(train), {OtherClassEval.metricName: \"f1\"}) for model in models]\n",
    "test_f1 = [OtherClassEval.evaluate(model.transform(test), {OtherClassEval.metricName: \"f1\"}) for model in models]\n",
    "\n",
    "training_1 = [OtherClassEval.evaluate(model.transform(train), {OtherClassEval.metricName: \"weightedPrecision\"}) for model in models]\n",
    "test_1 = [OtherClassEval.evaluate(model.transform(test), {OtherClassEval.metricName: \"weightedPrecision\"}) for model in models]\n",
    "\n",
    "training_2 = [OtherClassEval.evaluate(model.transform(train), {OtherClassEval.metricName: \"weightedRecall\"}) for model in models]\n",
    "test_2 = [OtherClassEval.evaluate(model.transform(test), {OtherClassEval.metricName: \"weightedRecall\"}) for model in models]\n",
    "\n",
    "training_3 = [OtherClassEval.evaluate(model.transform(train), {OtherClassEval.metricName: \"weightedTruePositiveRate\"}) for model in models]\n",
    "test_3 = [OtherClassEval.evaluate(model.transform(test), {OtherClassEval.metricName: \"weightedTruePositiveRate\"}) for model in models]\n",
    "\n",
    "training_4 = [OtherClassEval.evaluate(model.transform(train), {OtherClassEval.metricName: \"weightedFalsePositiveRate\"}) for model in models]\n",
    "test_4 = [OtherClassEval.evaluate(model.transform(test), {OtherClassEval.metricName: \"weightedFalsePositiveRate\"}) for model in models]\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model_Name\" : [\n",
    "        \"Ridge Logistic Regression\",\n",
    "        \"Lasso Logistic Regression\",\n",
    "        \"Naive Bayes\",\n",
    "        \"Random Forest\",\n",
    "        \"Gradient Boosted Tree\"\n",
    "    ],\n",
    "    'Training Accuracy' : training_acc,\n",
    "    'Test Accuracy' : test_acc,\n",
    "    'Training AUC' : training_auc,\n",
    "    'Test AUC' : test_auc,\n",
    "    'Training F1' : training_f1,\n",
    "    'Test F1' : test_f1,\n",
    "    'Training Precision' : training_1,\n",
    "    'Test Precision' : test_1,\n",
    "    'Training Recall' : training_2,\n",
    "    'Test Recall' : test_2,\n",
    "    'Training TPR' : training_3,\n",
    "    'Test TPR' : test_3,\n",
    "    'Training FPR' : training_4,\n",
    "    'Test FPR' : test_4\n",
    "}).sort_values('Test AUC').T\n",
    "\n",
    "results.columns = results.iloc[0,:].to_list()\n",
    "results.drop(index = \"Model_Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our table, it seems that overall both models performed around the same on the training and test set which is likely because we used crossvalidation.\n",
    "\n",
    "We found that the highest performing model in this case is the Gradient Boosted Tree model with a test AUC of 0.73."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "model_ridge.transform(test).plot_confusion_matrix('label', 'prediction')\n",
    "plt.title(\"Ridge Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "model_lasso.transform(test).plot_confusion_matrix('label', 'prediction')\n",
    "plt.title(\"Lasso Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "model_bayes.transform(test).plot_confusion_matrix('label', 'prediction')\n",
    "plt.title(\"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "model_rf.transform(test).plot_confusion_matrix('label', 'prediction')\n",
    "plt.title(\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "model_gbt.transform(test).plot_confusion_matrix('label', 'prediction')\n",
    "plt.title(\"Gradient Boosted Tree\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
